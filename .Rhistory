a=1
install.packages(openNLP)
install.packages("openNLP")
library openNLP
library("openNLP")
library()
library(openNLP)
library(openNLP)
library(rJAVA)
library(openNLP)
install.packages("rJAVA")
install.packages(rJAVA)
install.packages('rJAVA')
install.packages('rJava')
library(rJava)
install.packages('rJava')
library('rJava')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
library('rJava')
install.packages('rJava')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library(openNLP)
s <- "This is a sentence. This another---but with dash-like structures, and some commas.  Maybe another with question marks? Sure!"
sentDetect(s, language = "en")
library(openNLP)
sentDetect(s, language = "en", model = NULL)
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov. 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."),
collapse = "")
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- annotate(s, sent_token_annotator)
a1
## Extract sentences.
s[a1]
install.package('openNLPmodels.en')
install.packages('openNLPmodels.en')
s <- paste(c("This is a great phone"
collapse = "")
s <- paste(c("This is a great phone"))
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- annotate(s, sent_token_annotator)
a1
sta = Maxent_Sent_Token_Annotator()
sta
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library(openNLP)
require("NLP")
s = paste(c("This is a great phone"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
ea = Maxent_ENtity_Annotator()
ea = Maxent_Entity_Annotator()
install.packages("foo", repos = "http://datacube.wu.ac.at/", type = "source")
ea = Maxent_Entity_Annotator()
require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))pos_tag_annotator <- Maxent_POS_Tag_Annotator() pos_tag_annotator a3 <- annotate(s, pos_tag_annotator, a2) a3 ## Variant with POS tag probabilities as (additional) features. head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))## Determine the distribution of POS tags for word tokens. a3w <- subset(a3, type == "word") tags <- sapply(a3w$features, `[[`, "POS") tags table(tags) ## Extract token/POS pairs (all of them): easy. sprintf("%s/%s", s[a3w], tags)## Extract pairs of word tokens and POS tags for second sentence: a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]] sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))pos_tag_annotator <- Maxent_POS_Tag_Annotator() pos_tag_annotator a3 <- annotate(s, pos_tag_annotator, a2) a3 ## Variant with POS tag probabilities as (additional) features. head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))## Determine the distribution of POS tags for word tokens. a3w <- subset(a3, type == "word") tags <- sapply(a3w$features, `[[`, "POS") tags table(tags) ## Extract token/POS pairs (all of them): easy. sprintf("%s/%s", s[a3w], tags)## Extract pairs of word tokens and POS tags for second sentence: a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]] sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
## Variant with POS tag probabilities as (additional) features.
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
## Determine the distribution of POS tags for word tokens.
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
## Extract token/POS pairs (all of them): easy.
sprintf("%s/%s", s[a3w], tags)
## Extract pairs of word tokens and POS tags for second sentence:
a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]]
sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
## Requires package 'openNLPmodels.en' from the repository at ## <http://datacube.wu.ac.at>.require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))parse_annotator <- Parse_Annotator() ## Compute the parse annotations only. p <- parse_annotator(s, a2) ## Extract the formatted parse trees. ptexts <- sapply(p$features, `[[`, "parse") ptexts ## Read into NLP Tree objects. ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ptrees
ptrees <- lapply(ptexts, Tree_parse) ptrees]])ptrees
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
install.packages('openNLPmodels')
install.packages(openNLPmodels)
install.packages('OpenNLPmodels')
install.packages('OpenNLPmodel')
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
## Compute the parse annotations only.
p <- parse_annotator(s, a2)
## Extract the formatted parse trees.
ptexts <- sapply(p$features, `[[`, "parse")
ptexts
## Read into NLP Tree objects.
ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ea = Maxent_Entity_Annotator()
annotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)
#install.packages('rJava')
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
require("NLP")
## Some text.
s = paste(c("gives us a hero whose suffering and triumphs we can share"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
ea = Maxent_Entity_Annotator()
annotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)
## Requires package 'openNLPmodels.en' from the repository at
## <http://datacube.wu.ac.at>.
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
## Compute the parse annotations only.
p <- parse_annotator(s, a2)
## Extract the formatted parse trees.
ptexts <- sapply(p$features, `[[`, "parse")
ptexts
## Read into NLP Tree objects.
ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ptrees
library(rJava)
library(openNLP)
require("NLP")
s = paste(c("gives us a hero whose suffering and triumphs we can share"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
a2
score.sentiment(1,1,1)
getwd()
dir
dirpath = getwd()
dirpath
install.packages(sentiment)
install.packages("sentiment")
install.packages("semtiment")
install.packages("sentiment")
install.packages("rstem")
install.packages("Rstem")
install.packages("sentiment")
library(sentiment)
install.packages("wordcloud")
library(twitteR)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
install.packages("twitterR")
install.packages("twitteR")
install.packages("plyr")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("RColorBrewer")
install.packages("sentiment")
library(twitteR)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
install.packages("sentiment")
install.packages("sentiment140")
install.packages("tm.plugin.sentiment")
library(ss)
library(sos)
install.packages(sos)
install.packages("sos")
library(sos)
findFn('sentiment')
findFn('sentiment analysis')
install.packages("sentiment")
install.packages("qdap")
load(model, file="model_4mmrecordstraining.rda")
load(file="model_4mmrecordstraining.rda")
load(file="model_4mmrecordstraining.rda", model)
#INITIALIZE LIBRARIES
library(RODBC)
library("e1071")
library("Metrics")
library("rpart")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
load(file="model_4mmrecordstraining.rda", model)
load(file="model_4mmrecordstraining.rda")
#Test Data
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select id, I1, I2, I3, I4, I5, I6, I7, I8, I9, I10, I11, I12, I13,convert(bigint,convert(varbinary, c1)) as c1,convert(bigint,convert(varbinary, c2)) as c2, convert(bigint,convert(varbinary, c5)) as c5, convert(bigint,convert(varbinary, c6)) as c6, convert(bigint,convert(varbinary, c8)) as c8, convert(bigint,convert(varbinary, c9)) as c9, convert(bigint,convert(varbinary, c14)) as c14, convert(bigint,convert(varbinary, c17)) as c17, convert(bigint,convert(varbinary, c20)) as c20, convert(bigint,convert(varbinary, c22)) as c22, convert(bigint,convert(varbinary, c23)) as c23, convert(bigint,convert(varbinary, c25)) as c25 from test")
close(myconn)
test.input = sapply(test[15:26], as.numeric)
fit = kmeans(test.input, 10) # 5 cluster solution
#aggregate(test.input,by=list(fit$cluster),FUN=mean)
test <- cbind(test, fit$cluster)
glminc2 = c("I1", "I2", "I3", "I4", "I5", "I6", "I7", "I8", "I9", "I10", "I11", "I12", "I13", "fit.cluster")
test.input = test[glminc2]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\resultsES.csv", row.names = FALSE)
test <- cbind(test, fit$cluster)
remove(test.input)
remove(fit)
remove(myconn)
gc()
glminc2 = c("I1", "I2", "I3", "I4", "I5", "I6", "I7", "I8", "I9", "I10", "I11", "I12", "I13", "fit.cluster")
test.input = test[glminc2]
View(test)
test = test[0:28]
View(test)
test = test[0:27]
remove(test.input)
remove(fit)
remove(myconn)
gc()
glminc2 = c("I1", "I2", "I3", "I4", "I5", "I6", "I7", "I8", "I9", "I10", "I11", "I12", "I13", "fit.cluster")
test.input = test[glminc2]
View(test)
glminc2 = c("I1", "I2", "I3", "I4", "I5", "I6", "I7", "I8", "I9", "I10", "I11", "I12", "I13", "fit$cluster")
test.input = test[glminc2]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
names(test.input)["fit$cluster"]
names(test.input)[27] = "fit.cluster"
names(test.input)[14] = "fit.cluster"
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\resultsES.csv", row.names = FALSE)
View(Predicted.Test)
