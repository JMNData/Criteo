a=1
install.packages(openNLP)
install.packages("openNLP")
library openNLP
library("openNLP")
library()
library(openNLP)
library(openNLP)
library(rJAVA)
library(openNLP)
install.packages("rJAVA")
install.packages(rJAVA)
install.packages('rJAVA')
install.packages('rJava')
library(rJava)
install.packages('rJava')
library('rJava')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
library('rJava')
install.packages('rJava')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library(openNLP)
s <- "This is a sentence. This another---but with dash-like structures, and some commas.  Maybe another with question marks? Sure!"
sentDetect(s, language = "en")
library(openNLP)
sentDetect(s, language = "en", model = NULL)
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov. 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."),
collapse = "")
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- annotate(s, sent_token_annotator)
a1
## Extract sentences.
s[a1]
install.package('openNLPmodels.en')
install.packages('openNLPmodels.en')
s <- paste(c("This is a great phone"
collapse = "")
s <- paste(c("This is a great phone"))
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- annotate(s, sent_token_annotator)
a1
sta = Maxent_Sent_Token_Annotator()
sta
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library(openNLP)
require("NLP")
s = paste(c("This is a great phone"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
ea = Maxent_ENtity_Annotator()
ea = Maxent_Entity_Annotator()
install.packages("foo", repos = "http://datacube.wu.ac.at/", type = "source")
ea = Maxent_Entity_Annotator()
require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))pos_tag_annotator <- Maxent_POS_Tag_Annotator() pos_tag_annotator a3 <- annotate(s, pos_tag_annotator, a2) a3 ## Variant with POS tag probabilities as (additional) features. head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))## Determine the distribution of POS tags for word tokens. a3w <- subset(a3, type == "word") tags <- sapply(a3w$features, `[[`, "POS") tags table(tags) ## Extract token/POS pairs (all of them): easy. sprintf("%s/%s", s[a3w], tags)## Extract pairs of word tokens and POS tags for second sentence: a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]] sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))pos_tag_annotator <- Maxent_POS_Tag_Annotator() pos_tag_annotator a3 <- annotate(s, pos_tag_annotator, a2) a3 ## Variant with POS tag probabilities as (additional) features. head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))## Determine the distribution of POS tags for word tokens. a3w <- subset(a3, type == "word") tags <- sapply(a3w$features, `[[`, "POS") tags table(tags) ## Extract token/POS pairs (all of them): easy. sprintf("%s/%s", s[a3w], tags)## Extract pairs of word tokens and POS tags for second sentence: a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]] sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
## Variant with POS tag probabilities as (additional) features.
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
## Determine the distribution of POS tags for word tokens.
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
## Extract token/POS pairs (all of them): easy.
sprintf("%s/%s", s[a3w], tags)
## Extract pairs of word tokens and POS tags for second sentence:
a3ws2 <- annotations_in_spans(subset(a3, type == "word"), subset(a3, type == "sentence")[2L])[[1L]]
sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
## Requires package 'openNLPmodels.en' from the repository at ## <http://datacube.wu.ac.at>.require("NLP") ## Some text. s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "") s <- as.String(s)## Need sentence and word token annotations. sent_token_annotator <- Maxent_Sent_Token_Annotator() word_token_annotator <- Maxent_Word_Token_Annotator() a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))parse_annotator <- Parse_Annotator() ## Compute the parse annotations only. p <- parse_annotator(s, a2) ## Extract the formatted parse trees. ptexts <- sapply(p$features, `[[`, "parse") ptexts ## Read into NLP Tree objects. ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ptrees
ptrees <- lapply(ptexts, Tree_parse) ptrees]])ptrees
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
install.packages('openNLPmodels')
install.packages(openNLPmodels)
install.packages('OpenNLPmodels')
install.packages('OpenNLPmodel')
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
## Compute the parse annotations only.
p <- parse_annotator(s, a2)
## Extract the formatted parse trees.
ptexts <- sapply(p$features, `[[`, "parse")
ptexts
## Read into NLP Tree objects.
ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ea = Maxent_Entity_Annotator()
annotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)
#install.packages('rJava')
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
require("NLP")
## Some text.
s = paste(c("gives us a hero whose suffering and triumphs we can share"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
ea = Maxent_Entity_Annotator()
annotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)
## Requires package 'openNLPmodels.en' from the repository at
## <http://datacube.wu.ac.at>.
require("NLP")
## Some text.
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ", "nonexecutive director Nov. 29.\n", "Mr. Vinken is chairman of Elsevier N.V., ", "the Dutch publishing group."), collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
## Compute the parse annotations only.
p <- parse_annotator(s, a2)
## Extract the formatted parse trees.
ptexts <- sapply(p$features, `[[`, "parse")
ptexts
## Read into NLP Tree objects.
ptrees <- lapply(ptexts, Tree_parse) ptrees]])
ptrees
library(rJava)
library(openNLP)
require("NLP")
s = paste(c("gives us a hero whose suffering and triumphs we can share"))
s = as.String(s)
sta = Maxent_Sent_Token_Annotator()
wta = Maxent_Word_Token_Annotator()
a2 = annotate(s,list(sta,wta))
a2
score.sentiment(1,1,1)
getwd()
dir
dirpath = getwd()
dirpath
install.packages(sentiment)
install.packages("sentiment")
install.packages("semtiment")
install.packages("sentiment")
install.packages("rstem")
install.packages("Rstem")
install.packages("sentiment")
library(sentiment)
install.packages("wordcloud")
library(twitteR)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
install.packages("twitterR")
install.packages("twitteR")
install.packages("plyr")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("RColorBrewer")
install.packages("sentiment")
library(twitteR)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
install.packages("sentiment")
install.packages("sentiment140")
install.packages("tm.plugin.sentiment")
library(ss)
library(sos)
install.packages(sos)
install.packages("sos")
library(sos)
findFn('sentiment')
findFn('sentiment analysis')
install.packages("sentiment")
install.packages("qdap")
x = 1
remote(x)
remove(x)
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 * from train")
close(myconn)
train.input = train[2:41]
model = glm(Label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
print(model)
summary(model)
test.input = test[2:15]
test = sqlQuery(myconn, "select top 50 * from test")
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select top 50 * from test")
test.input = test[2:15]
View(test.input)
test.input = test[2:14]
View(test.input)
Predicted.Test = cbind(test, predict = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("Label", "predict")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
View(Predicted.Test)
View(test)
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
View(Predicted.Test)
out = c("Id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
close(myconn)
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 * from train")
close(myconn)
train.input = train[2:41]
model = glm(Label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
print(model)
summary(model)
train = sqlQuery(myconn, "select top 100000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
close(myconn)
train.input = train[2:41]
model = glm(Label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
print(model)
summary(model)
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 1000000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
close(myconn)
train.input = train[2:41]
model = glm(Label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
print(model)
summary(model)
View(train)
train = sqlQuery(myconn, "select top 1000000 id, label, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 1000000 id, label, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
close(myconn)
train.input = train[2:14]
View(train.input)
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 1000000 id, label, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
close(myconn)
train.input = train[2:14]
model = glm(Label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
train.input = train[2:15]
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
train = sqlQuery(myconn, "select top 100000 id, label, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 id, label, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from train")
close(myconn)
train.input = train[2:15]
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
train = sqlQuery(myconn, "select top 100000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 100000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
test.input = test[2:14]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("Id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
close(myconn)
View(Predicted.Test)
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
library("parallel")
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
library("parallel")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
#model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
model = randomForest(count~., data=train)
library("randomForest")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
#model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
model = randomForest(count~., data=train)
model = randomForest(label~., data=train)
model.rpart = rpart(label~., data=Train1)
model.rpart = rpart(label~., data=train)
library("rpart")
model.rpart = rpart(label~., data=train)
print(model.rpart)
rsq.rpart(model.rpart)
model = randomForest(label~., data=train)
summary(model.rpart)
printcp(model.rpart)
plot(model.rpart)
plot(model)
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
library("parallel")
library("randomForest")
library("rpart")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
#train = sqlQuery(myconn, "select top 10000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
train = sqlQuery(myconn, "select top 10000 * from train")
close(myconn)
train.input = train[2:15]
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
library("parallel")
library("randomForest")
library("rpart")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
#train = sqlQuery(myconn, "select top 10000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
train = sqlQuery(myconn, "select top 10000 * from train")
close(myconn)
train.input = train[2:15]
model = randomForest(label~., data=train)
model = randomForest(Label~., data=train)
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
close(myconn)
test.input = test[2:14]
test = sqlQuery(myconn, "select top 1000 * from test")
myconn = odbcConnect("Citeo")
#test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
test = sqlQuery(myconn, "select top 1000 * from test")
close(myconn)
test.input = test[2:14]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
model = randomForest(Label~., data=train)
model = rpart(label~., data=train)
model = rpart(Label~., data=train)
summary(model)
myconn = odbcConnect("Citeo")
#test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
test = sqlQuery(myconn, "select top 1000 * from test")
close(myconn)
test.input = test[2:14]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
predict(model, test.input, interval="predict", type="response")
predict(model, test.input, interval="predict")
predict(model, test.input, type="response")
View(test.input)
test.input = test[2:41]
test.input = test[2:40]
predict(model, test.input, type="response")
summary(model)
myconn = odbcConnect("Citeo")
#test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
test = sqlQuery(myconn, "select top 1000 * from test")
close(myconn)
test.input = test[2:40]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
#install.packages("RODBC")
#INITIALIZE LIBRARIES
library(RODBC)
library("parallel")
library("randomForest")
library("rpart")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 10000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
model = glm(label~I1+I2+I3+I4+I5+I6+I7+I8+I9+I10+I11+I12+I13, data=train.input, family=binomial())
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
close(myconn)
test.input = test[2:40]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
myconn = odbcConnect("Citeo")
test = sqlQuery(myconn, "select top 1000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13 from test")
close(myconn)
test.input = test[2:14]
Predicted.Test = cbind(test, predicted = round(predict(model, test.input, interval="predict", type="response"), digits=10))
out = c("id", "predicted")
write.csv(Predicted.Test[out], "data\\results.csv", row.names = FALSE)
#INITIALIZE LIBRARIES
library(RODBC)
library("e1071")
library("Metrics")
library("rpart")
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 10000 id, I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13, label from train")
close(myconn)
train.input = train[2:15]
gc()
#Set Globals
setwd("C:\\Users\\Administrator\\Documents\\GitHub\\Criteo")
#GET DATA
myconn = odbcConnect("Citeo")
train = sqlQuery(myconn, "select top 10000 * from train")
close(myconn)
train.input = train[2:15]
